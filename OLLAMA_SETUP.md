# Ollama Setup –¥–ª—è Local Agent

## ‚úÖ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞

**Ollama –≤–µ—Ä—Å–∏—è**: 0.14.2  
**–ü—É—Ç—å**: `C:\Users\-\AppData\Local\Programs\Ollama`  
**–°–µ—Ä–≤–µ—Ä**: http://localhost:11434

## üì¶ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

- **Gemma 3:4b** - 3.3 GB (—É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞)
- **Qwen 2.5 Coder 1.5B** - –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è...

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –î–æ–±–∞–≤–∏—Ç—å Ollama –≤ PATH (—Ç–µ–∫—É—â–∞—è —Å–µ—Å—Å–∏—è)

```powershell
.\scripts\setup-ollama.ps1
```

### 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å—Ç–∞–Ω–æ–≤–∫—É

```powershell
ollama --version
ollama list
```

### 3. –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏ Qwen

```powershell
# –õ–µ–≥–∫–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ (1.5B, ~1GB)
ollama pull qwen2.5-coder:1.5b

# –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞ (7B, ~4.7GB)
ollama pull qwen2.5-coder:7b

# –ë–∞–∑–æ–≤–∞—è Qwen (7B, ~4.7GB)
ollama pull qwen2:7b

# –°–æ–≤—Å–µ–º –ª–µ–≥–∫–∞—è (0.5B, ~397MB)
ollama pull qwen2:0.5b
```

### 4. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```powershell
# –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç
ollama run qwen2.5-coder:1.5b "Write a hello world in Python"

# –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
ollama run qwen2.5-coder:1.5b
```

## üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Local Agent

### –í–∞—Ä–∏–∞–Ω—Ç 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Ollama –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º Local Agent

**–í `C:\local-agent\`:**

–û–±–Ω–æ–≤–∏—Ç–µ `agent/llm.py` –∏–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ `agent/llm_ollama.py`:

```python
import httpx
import json

class OllamaLLM:
    def __init__(self, model="qwen2.5-coder:1.5b"):
        self.model = model
        self.base_url = "http://localhost:11434"
        self.client = httpx.AsyncClient(timeout=300.0)
    
    async def generate(self, prompt, **kwargs):
        response = await self.client.post(
            f"{self.base_url}/api/generate",
            json={
                "model": self.model,
                "prompt": prompt,
                "stream": False
            }
        )
        return response.json()["response"]
```

### –í–∞—Ä–∏–∞–Ω—Ç 2: –ù–æ–≤—ã–π Local Agent —Å Ollama

**–í `C:\Users\-\Desktop\game-store\local-agent\`:**

```powershell
cd local-agent
pip install httpx
```

–°–æ–∑–¥–∞–π—Ç–µ `agent/llm_ollama.py` (—É–∂–µ –≥–æ—Ç–æ–≤ –≤ –ø—Ä–æ–µ–∫—Ç–µ)

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Qwen

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | VRAM | –°–∫–æ—Ä–æ—Å—Ç—å | –ö–∞—á–µ—Å—Ç–≤–æ | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ |
|--------|--------|------|----------|----------|---------------|
| **qwen2:0.5b** | 397MB | ~1GB | –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ | –ë–∞–∑–æ–≤–æ–µ | –¢–µ—Å—Ç—ã, –ø—Ä–æ—Ç–æ—Ç–∏–ø—ã |
| **qwen2.5-coder:1.5b** | ~1GB | ~2GB | –ë—ã—Å—Ç—Ä–æ | –•–æ—Ä–æ—à–µ–µ | –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ |
| **qwen2:7b** | 4.7GB | ~8GB | –°—Ä–µ–¥–Ω–µ | –û—Ç–ª–∏—á–Ω–æ–µ | –û–±—â–∏–µ –∑–∞–¥–∞—á–∏ |
| **qwen2.5-coder:7b** | 4.7GB | ~8GB | –°—Ä–µ–¥–Ω–µ | –û—Ç–ª–∏—á–Ω–æ–µ | Coding |
| **qwen2.5-coder:14b** | ~9GB | ~16GB | –ú–µ–¥–ª–µ–Ω–Ω–æ | –ü—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–µ | Production |

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### –î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏:
```powershell
ollama pull qwen2.5-coder:1.5b
```

### –î–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞:
```powershell
ollama pull qwen2.5-coder:7b
```

### –î–ª—è —Å–ª–∞–±—ã—Ö —Å–∏—Å—Ç–µ–º:
```powershell
ollama pull qwen2:0.5b
```

## üîß –ö–æ–º–∞–Ω–¥—ã Ollama

### –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏

```powershell
# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
ollama list

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
ollama pull <model>

# –£–¥–∞–ª–∏—Ç—å –º–æ–¥–µ–ª—å
ollama rm <model>

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
ollama show <model>
```

### –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏

```powershell
# –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
ollama run qwen2.5-coder:1.5b

# –û–¥–Ω–æ—Ä–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
ollama run qwen2.5-coder:1.5b "Your prompt here"

# –° –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
ollama run qwen2.5-coder:1.5b --temperature 0.7 "Your prompt"
```

### –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–µ—Ä–≤–µ—Ä–æ–º

```powershell
# –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–µ—Ä (–æ–±—ã—á–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
ollama serve

# –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–µ—Ä–≤–µ—Ä
# –ù–∞–π—Ç–∏ –ø—Ä–æ—Ü–µ—Å—Å: Get-Process ollama
# –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å: Stop-Process -Name ollama
```

## üåê API

Ollama –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç REST API –Ω–∞ `http://localhost:11434`

### –û—Å–Ω–æ–≤–Ω—ã–µ endpoints:

**POST /api/generate** - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
```json
{
  "model": "qwen2.5-coder:1.5b",
  "prompt": "Write a function",
  "stream": false
}
```

**POST /api/chat** - –ß–∞—Ç —Å –∏—Å—Ç–æ—Ä–∏–µ–π
```json
{
  "model": "qwen2.5-coder:1.5b",
  "messages": [
    {"role": "user", "content": "Hello"}
  ]
}
```

**GET /api/tags** - –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π

**POST /api/pull** - –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å

## üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø—Ä–æ–µ–∫—Ç–∞–º–∏

### Node.js / TypeScript

```typescript
async function queryOllama(prompt: string) {
  const response = await fetch('http://localhost:11434/api/generate', {
    method: 'POST',
    body: JSON.stringify({
      model: 'qwen2.5-coder:1.5b',
      prompt,
      stream: false
    })
  });
  
  const data = await response.json();
  return data.response;
}
```

### Python

```python
import httpx

async def query_ollama(prompt):
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "qwen2.5-coder:1.5b",
                "prompt": prompt,
                "stream": False
            }
        )
        return response.json()["response"]
```

## üìù –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å Local Agent

### –ó–∞–ø—É—Å–∫ Local Agent —Å Ollama

```powershell
cd C:\local-agent

# –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω
ollama serve

# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–≥–µ–Ω—Ç–∞
python run.py --llm ollama --model qwen2.5-coder:1.5b
```

### –ò–ª–∏ —á–µ—Ä–µ–∑ API —Å–µ—Ä–≤–µ—Ä

```powershell
python server.py
```

–ó–∞—Ç–µ–º –≤ –¥—Ä—É–≥–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:
```powershell
curl -X POST http://localhost:8000/query `
  -H "Content-Type: application/json" `
  -d '{"query": "List Python files", "llm": "ollama"}'
```

## üêõ Troubleshooting

### Ollama –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è

```powershell
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å
Get-Process ollama

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å
Stop-Process -Name ollama -Force
ollama serve
```

### –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞

```powershell
ollama list  # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
ollama pull qwen2.5-coder:1.5b  # –ó–∞–≥—Ä—É–∑–∏—Ç—å
```

### –ú–µ–¥–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è

- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å (1.5B –≤–º–µ—Å—Ç–æ 7B)
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ VRAM: `nvidia-smi` (–µ—Å–ª–∏ GPU)
- –£–º–µ–Ω—å—à–∏—Ç–µ `max_tokens`

### –ù–µ –º–æ–∂–µ—Ç –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ —Å–µ—Ä–≤–µ—Ä—É

```powershell
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∑–∞–ø—É—â–µ–Ω –ª–∏ —Å–µ—Ä–≤–µ—Ä
Invoke-WebRequest -Uri "http://localhost:11434/api/tags" -UseBasicParsing

# –ï—Å–ª–∏ –Ω–µ –∑–∞–ø—É—â–µ–Ω, –∑–∞–ø—É—Å—Ç–∏—Ç—å:
ollama serve
```

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- **Ollama**: https://ollama.com/
- **Qwen Models**: https://ollama.com/library/qwen2.5-coder
- **API Docs**: https://github.com/ollama/ollama/blob/main/docs/api.md
- **Model Library**: https://ollama.com/library

## ‚ú® –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Ollama

- ‚úÖ –ü—Ä–æ—â–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ (–±–µ–∑ CUDA Toolkit)
- ‚úÖ –õ–µ–≥—á–µ –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏
- ‚úÖ REST API –∏–∑ –∫–æ—Ä–æ–±–∫–∏
- ‚úÖ –ú–µ–Ω—å—à–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- ‚úÖ –õ—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ CPU
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π

## üîÑ Ollama vs ExLlamaV2

| –ö—Ä–∏—Ç–µ—Ä–∏–π | Ollama | ExLlamaV2 |
|----------|--------|-----------|
| **–£—Å—Ç–∞–Ω–æ–≤–∫–∞** | –ü—Ä–æ—Å—Ç–∞—è | –°–ª–æ–∂–Ω–∞—è (CUDA) |
| **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ** | –õ–µ–≥–∫–æ–µ (API) | –¢—Ä–µ–±—É–µ—Ç –∫–æ–¥–∞ |
| **–°–∫–æ—Ä–æ—Å—Ç—å (GPU)** | –•–æ—Ä–æ—à–∞—è | –û—Ç–ª–∏—á–Ω–∞—è |
| **–°–∫–æ—Ä–æ—Å—Ç—å (CPU)** | –û—Ç–ª–∏—á–Ω–∞—è | –ú–µ–¥–ª–µ–Ω–Ω–∞—è |
| **–ú–æ–¥–µ–ª–∏** | –ì–æ—Ç–æ–≤—ã–µ | –ù—É–∂–Ω–æ –∫–∞—á–∞—Ç—å |
| **–ü–∞–º—è—Ç—å** | –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è | –ë–æ–ª–µ–µ —Ç—Ä–µ–±–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Ollama –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏, ExLlamaV2 - –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ GPU.

---

**Ollama –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ! üéâ**

–î–ª—è –Ω–∞—á–∞–ª–∞:
```powershell
ollama run qwen2.5-coder:1.5b "Write a hello world in Python"
```
